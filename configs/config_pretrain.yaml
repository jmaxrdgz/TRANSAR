# config_pretrain.yaml
# SimMIM Pretraining Configuration

SEED: 42

MODEL:
  # Backbone selection - choose from:
  # Swin v1: swin_tiny, swin_small
  # Swin v2: swinv2_tiny_w8, swinv2_tiny_w16, swinv2_small_w8, swinv2_small_w16
  BACKBONE: swinv2_tiny_w8

  IN_CHANS: 1                    # 1 for SAR, 3 for RGB
  MASK_RATIO: 0.6                # Proportion of patches to mask (SimMIM default: 0.6)


  # Block-wise masking (TRANSAR paper)
  # Instead of masking random individual patches, mask contiguous blocks
  # TRANSAR found mask_size=8 gives best results (Section C.1)
  # - mask_size=4: too small, creates blurry reconstructions
  # - mask_size>8: too large, ignores background variations
  MASK_SIZE: 8                   # Size of block to mask (8x8 blocks of patches)

  RESUME: null                   # Path to checkpoint to resume from

DATA:
  # Input size must satisfy:
  # 1. Divisible by patch_size (4)
  # 2. (img_size / patch_size) divisible by window_size
  # 3. (img_size / patch_size) divisible by mask_size for block-wise masking
  # For window_size=8, patch_size=4, mask_size=8: 512 is valid (512/4=128, 128%8=0, 128%8=0)
  IMG_SIZE: 512

  # Dataset format: 'npy' for individual files, 'hdf5' for single HDF5 file
  # - 'npy': Individual .npy files per chip (simple, good for small datasets)
  # - 'hdf5': Single HDF5 file with all chips (faster I/O, good for large datasets)
  FORMAT: npy

  TRAIN_DATA: dataset/pretrain/unlabeled  # For npy: directory, for hdf5: .h5 file path
  NUM_WORKERS: 8

  # SAR normalization: x_norm = (x - μ_c) / σ_g
  # Set to null to disable normalization
  # Use scripts/compute_global_std.py to compute from your training data
  GLOBAL_STD: null              # Set this to the global std of your training dataset

  # Optional: Capella-specific logarithmic normalization
  # Set to 16 for Capella SAR data as used in TRANSAR paper
  # Set to null for other SAR data (default)
  # NOTE: Do NOT apply log2 normalization during chip preprocessing if using this
  S_NORM: null                  # Normalization scale constant for log2(x) / s_norm

TRAIN:
  BATCH_SIZE: 16                # 16 in TRANSAR paper
  EPOCHS: 100                   # 100 for TRANSAR pretraining
  LR: 0.001                     # Base learning rate (scaled with batch size)
  WEIGHT_DECAY: 0.05            # AdamW weight decay
  WARMUP_EPOCHS: 10             # Linear warmup epochs
  CLIP_GRAD: 5.0                # Gradient clipping
  N_GPU: 2                      # Number of GPUs

# Experimentation guide:
#
# Test different backbones:
#   MODEL.BACKBONE: swin_tiny / swinv2_tiny_w8 / swinv2_tiny_w16
#
# Test different input sizes (must be compatible with window size):
#   For window=8: DATA.IMG_SIZE: 192 / 256 / 320 / 384
#   For window=16: DATA.IMG_SIZE: 256 / 320 / 384 / 512
#
# Test different mask ratios:
#   MODEL.MASK_RATIO: 0.4 / 0.6 / 0.75
#
# Adjust batch size for your GPU memory:
#   TRAIN.BATCH_SIZE: 64 / 128 / 256
#
# Switch between dataset formats:
#   DATA.FORMAT: npy  # Individual .npy files
#   DATA.FORMAT: hdf5 # Single HDF5 file (faster for large datasets)
#
# CLI override examples:
#   python pretrain.py --override MODEL.BACKBONE=swinv2_tiny_w16
#   python pretrain.py --override DATA.IMG_SIZE=384 TRAIN.BATCH_SIZE=64
#   python pretrain.py --override MODEL.MASK_SIZE=4 MODEL.MASK_RATIO=0.5
#   python pretrain.py --override DATA.FORMAT=hdf5 DATA.TRAIN_DATA=dataset/chips.h5

